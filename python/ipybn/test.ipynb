{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proxy: {'http': 'http://119.101.115.75:9999'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "proxyHttp = [\n",
    "    {'http': 'http://119.101.115.75:9999'},\n",
    "    {'http': 'http://223.215.101.254:9999'}\n",
    "]\n",
    "proxyHttps = [\n",
    "    {'https': 'https://139.129.207.72:808'},\n",
    "    {'https': 'https://119.101.116.219:9999'}\n",
    "]\n",
    "ua1 = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "ua2 = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'\n",
    "head = {'User-Agent': ua2, \n",
    "             'Connection': 'keep-alive'}\n",
    "\n",
    "url = 'https://sh.zu.anjuke.com/?pi=baidu-cpchz-sh-hexin1&kwid=63651556880&utm_term=%E7%A7%9F%E6%88%BF'\n",
    "url1 = 'http://139.224.115.177:7777'\n",
    "url2 = 'https://icanhazip.com'\n",
    "url3 = 'https://www.toutiao.com/'\n",
    "#proxy = random.choice(proxyHttp)\n",
    "#print('proxy:',proxy)\n",
    "proxy = proxyHttp[0]\n",
    "p = requests.get(url1, headers=head, proxies = proxy)\n",
    "# p = requests.get(url3, headers=head)\n",
    "# print(p.text)\n",
    "# print(p.select('.single-mode-rbox'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最简单的爬虫程序demo , 适用于静态的,没有防爬虫策略的网站\n",
    "\n",
    "import requests # 发送网络请求，获取响应内容\n",
    "from bs4 import BeautifulSoup  # 对于网站响应的html进行解析 和数据的清洗\n",
    "import pandas # 数据可视化，保存数据\n",
    "\n",
    "def simpleDemo():\n",
    "    url = 'https://news.qudong.com/yejie/' # 要爬取的网页的网站\n",
    "\n",
    "    # 1 获取网页内容\n",
    "    req = requests.get(url) # 进行网络请求，获取响应的内容\n",
    "    req.encoding = 'utf8' # 设置解析编码,防止中文乱码\n",
    "    contentAll = req.text # 解析为文本\n",
    "\n",
    "    # 2 清洗数据，处理\n",
    "    soup = BeautifulSoup(contentAll,'html.parser') # 创建 beautifulsoup对象\n",
    "    contentList = soup.select('.text h4 ') # 根据css帅选数据,得到一个嵌套着beautifulsoup对象的列表\n",
    "    # 为了更好的进行数据的保存与展示，将数据保存在嵌套字典的列表里\n",
    "    all = [] # \n",
    "    for item in contentList: # 循环获取文本数据\n",
    "        all.append({'title':item.text}) \n",
    "\n",
    "    # 3 保存数据\n",
    "    # 使用pandas库函数，将数据导入excel\n",
    "    all = pandas.DataFrame(all)\n",
    "    all.to_excel('./temp/content.xlsx')\n",
    "    # print(all)\n",
    "simpleDemo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加代理\n",
    "import requests\n",
    "from bs4 import BeautifulSoup \n",
    "import pandas \n",
    "\n",
    "def addProxy():\n",
    "    # 设置代理ip，防止本地ip被封\n",
    "    proxyHttps = {'https': 'https://139.129.207.72:808'}\n",
    "    # 设置ua,伪装成游览器\n",
    "    userAgent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'\n",
    "    head = {'User-Agent': userAgent,\n",
    "            'Connection': 'keep-alive'}\n",
    "\n",
    "    # 1 获取网页内容\n",
    "    # req = requests.get(url3, headers=head, proxies = proxy)  # 免费的ip代理，速度比较慢，我就不用了\n",
    "    url = 'https://sh.lianjia.com/ershoufang/?utm_source=baidu&utm_medium=pinzhuan&utm_term=biaoti&utm_content\\\n",
    "            =biaotimiaoshu&utm_campaign=sousuo&ljref=pc_sem_baidu_ppzq_x'\n",
    "    req = requests.get(url, headers=head)\n",
    "    req.encoding = 'utf8' # 设置解析编码,防止中文乱码\n",
    "    contentAll = req.text # 解析为文本\n",
    "\n",
    "    # 2 清洗数据，处理\n",
    "    soup = BeautifulSoup(contentAll,'html.parser') # 创建 beautifulsoup对象\n",
    "    contentList = soup.select('.info.clear .title a') # 根据css帅选数据,得到一个嵌套着beautifulsoup对象的列表\n",
    "\n",
    "    all = []\n",
    "    for item in contentList:\n",
    "        all.append({'title':item.text})\n",
    "\n",
    "    # 3 保存数据\n",
    "    all = pandas.DataFrame(all)\n",
    "    all.to_excel('./temp/content.xlsx')\n",
    "    # print(all)\n",
    "addProxy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 终结篇\n",
    "# 说明：\n",
    "# 1. selenium是一款用来做自动化测试的工具，可以通过加载游览器的驱动控制游览器，在这里用来控制游览器访问网页爬取数据\n",
    "\n",
    "# 2. User Agent 是根据 操作系统，CPU，游览器类型 形成的\n",
    "\n",
    "# 模拟游览器爬取动态网页\n",
    "# !pip install selenium\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "import random\n",
    "import pandas\n",
    "def final():\n",
    "    driverPath = 'D:/Python/chromedriver.exe'\n",
    "    userAgent1 = 'Mozilla/5.0(compatible;MSIE9.0;WindowsNT6.1;Trident/5.0'\n",
    "    # OPPO A57\t Android\t手机百度\n",
    "    userAgent2 = 'Mozilla/5.0 (Linux; Android 6.0.1; OPPO A57 Build/MMB29M; wv) AppleWebKit/537.36(KHTML, like Gecko) Version/4.0 Chrome/63.0.3239.83 Mobile Safari/537.36 T7/10.13 baiduboxapp/10.13.0.10 (Baidu; P1 6.0.1)'\n",
    "\n",
    "    chromeOptions = Options()\n",
    "    # 设置游览器代理\n",
    "    chromeOptions.add_argument('user-agent=' + userAgent1)\n",
    "    chromeOptions.add_argument('disable-infobars') \n",
    "    \n",
    "    # 设置不加载图片\n",
    "    prefs = {\"profile.managed_default_content_settings.images\": 2} \n",
    "    chromeOptions.add_experimental_option(\"prefs\", prefs) \n",
    "    \n",
    "    # 不显示游览器\n",
    "    chromeOptions.add_argument('--headless') \n",
    "    chromeOptions.add_argument('--disable-gpu')\n",
    "    \n",
    "    # 设置ip代理\n",
    "    ipProxy = 'http://110.52.235.99:9999'\n",
    "    chromeOptions.add_argument(\"--proxy-server=\" + ipProxy)\n",
    "    browser = webdriver.Chrome(executable_path = driverPath, options = chromeOptions) # 获取游览器的驱动\n",
    "    browser.implicitly_wait(10)  # 隐式等待10 s，即最长等待元素加载时间为10s并且一旦发现元素加载成功则执行，全局有效\n",
    "    url2 = 'http://139.224.115.177:7777'\n",
    "    url3 = 'http://httpbin.org/ip'\n",
    "    url4 = 'https://www.toutiao.com/'\n",
    "    browser.get(url2)\n",
    "    return\n",
    "    # 循环下拉加载10次\n",
    "    height = browser.execute_script('return screen.height')\n",
    "    for _ in range(10):\n",
    "        sleepTime = random.uniform(1.1,3.0) # 生成浮点型随机数\n",
    "        time.sleep(sleepTime) # 强制等待，防止加载过快，触发服务端防爬虫机制\n",
    "        js='var q=document.documentElement.scrollTop=' + str(height)\n",
    "        browser.execute_script(js)\n",
    "        height += 700\n",
    "    # content = browser.find_elements_by_css_selector('.link.title')\n",
    "    content = browser.find_elements_by_xpath(\"//a[@class='link title']\")\n",
    "    for item in content:\n",
    "        print(content.index(item)+1,'--',item.text.replace(' ','').replace('\\n',''))\n",
    "    browser.close() # 关闭浏览器\n",
    "    browser.quit() # 关闭chreomedriver进程\n",
    "final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrome_options.add_argument('--headless') 不显示游览器\n",
    "# chrome_options.add_argument('--disable-gpu')\n",
    "# chrome_options.add_argument('disable-infobars') \n",
    "\n",
    "# html = browser.execute_script(\"return document.documentElement.outerHTML\") 执行js代码\n",
    "# html = browser.find_elements_by_css_selector('.main-list')[0].get_attribute('innerHTML') 获取html\n",
    "\n",
    "# 不加载图片\n",
    "# prefs = {\"profile.managed_default_content_settings.images\": 2}  \n",
    "# chrome_options.add_experimental_option(\"prefs\", prefs) \n",
    "\n",
    "# options.setCapability(\"proxy\", proxy) 设置代理\n",
    "# driver.find_element_by_id('').send_keys('') 填写表单\n",
    "# stime = driver.find_elements_by_css_selector('.c_tx.c_tx3.goDetail') css选择器\n",
    "# driver.find_element_by_link_text('下一页').click() 通过文本选择\n",
    "# driver.switch_to.frame('app_canvas_frame') 进入网页内置的ifrane\n",
    "# driver.save_screenshot(\"baidu.png\") 生成网页图片\n",
    "# driver.page_source 获取网页源码\n",
    "# driver.get_cookies() \n",
    "# from selenium.webdriver.common.action_chains import ActionChains  是一个底层的自动交互的方法库，例如鼠标移动、鼠标按键事件、键盘响应和菜单右击交互。\n",
    "\n",
    "# driver.find_element_by_id(\"kw\").send_keys(Keys.CONTROL, 'a') ctrl+a 全选输入框内容\n",
    "# driver.find_element_by_id(\"kw\").send_keys(Keys.CONTROL, 'x') ctrl+x 剪切输入框内容\n",
    "\n",
    "# from selenium.webdriver.common.keys import Keys 键盘按键操作的keys包\n",
    "# driver.find_element_by_id(\"su\").send_keys(Keys.RETURN) 模拟Enter回车键\n",
    "\n",
    "# from selenium.webdriver.support import expected_conditions as EC  selenium的判断模块，例如判断是否存在某个元素\n",
    "# from selenium.webdriver.support.wait import WebDriverWait 显示等待模块\n",
    "\n",
    "# js\n",
    "# window.screen.availHeight 返回当前屏幕高度(空白空间) \n",
    "# window.screen.height 返回当前屏幕高度(分辨率值) \n",
    "# document.documentElement.clientHeight => 就是网页在浏览器中可见高度，不包括浏览器自身的状态栏，随着浏览器大小变化；\n",
    "# window.document.body.offsetHeight; 返回当前网页高度 \n",
    "# document.body.scrollHeight 返回当前网页高度 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
